{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def connect_to_db() -> psycopg2.extensions.connection:\n",
    "    return psycopg2.connect(\n",
    "        host=os.getenv(\"DB_HOST\"),\n",
    "        port=os.getenv(\"DB_PORT\"),\n",
    "        database=os.getenv(\"DB_NAME\"),\n",
    "        user=os.getenv(\"DB_USER\"),\n",
    "        password=os.getenv(\"DB_PASSWORD\"),\n",
    "    )\n",
    "\n",
    "def read_gnd_mappings(file_path: str) -> tuple:\n",
    "    \"\"\"Read GND mappings from CSV file and return unique values\"\"\"\n",
    "    print(\"Reading CSV file...\")\n",
    "    # Read CSV and immediately drop duplicates\n",
    "    df = pd.read_csv(file_path).drop_duplicates()\n",
    "    \n",
    "    # Get unique doc_idns\n",
    "    doc_idns = df['doc_idn'].unique().tolist()\n",
    "    \n",
    "    # Get unique GND entities\n",
    "    gnd_entities = df[['gnd_idn', 'gnd_label']].drop_duplicates().values.tolist()\n",
    "    \n",
    "    # Get mappings (now without duplicates)\n",
    "    mappings = df[['doc_idn', 'gnd_idn']].values.tolist()\n",
    "    \n",
    "    print(f\"Found {len(doc_idns)} unique documents\")\n",
    "    print(f\"Found {len(gnd_entities)} unique GND entities\")\n",
    "    print(f\"Found {len(mappings)} total mappings (after removing duplicates)\")\n",
    "    \n",
    "    return doc_idns, gnd_entities, mappings\n",
    "\n",
    "def create_gnd_tables(cur: psycopg2.extensions.cursor) -> None:\n",
    "    \"\"\"Create GND tables with proper constraints\"\"\"\n",
    "    print(\"Dropping existing tables...\")\n",
    "    cur.execute(\"\"\"\n",
    "        DROP TABLE IF EXISTS dnb_gnd_mappings;\n",
    "        DROP TABLE IF EXISTS dnb_gnd_entities;\n",
    "        DROP TABLE IF EXISTS dnb_records_gnd;\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Creating new tables...\")\n",
    "    # Create records subset table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE dnb_records_gnd AS \n",
    "        SELECT * FROM dnb_records WHERE FALSE;\n",
    "        ALTER TABLE dnb_records_gnd ADD PRIMARY KEY (id);\n",
    "        CREATE INDEX idx_records_gnd_idn ON dnb_records_gnd(idn);\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create GND entities table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE dnb_gnd_entities (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            gnd_idn VARCHAR NOT NULL UNIQUE,\n",
    "            gnd_label TEXT NOT NULL\n",
    "        );\n",
    "        CREATE INDEX idx_gnd_entities_idn ON dnb_gnd_entities(gnd_idn);\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create mappings table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE dnb_gnd_mappings (\n",
    "            record_id INTEGER REFERENCES dnb_records_gnd(id),\n",
    "            gnd_id INTEGER REFERENCES dnb_gnd_entities(id),\n",
    "            PRIMARY KEY (record_id, gnd_id)\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "def populate_tables(cur: psycopg2.extensions.cursor, doc_idns: list, gnd_entities: list, mappings: list) -> None:\n",
    "    \"\"\"Populate all tables efficiently using batch operations\"\"\"\n",
    "    # Check if we need to populate GND entities\n",
    "    cur.execute(\"SELECT COUNT(*) FROM dnb_gnd_entities\")\n",
    "    current_count = cur.fetchone()[0]\n",
    "    \n",
    "    if current_count != len(gnd_entities):\n",
    "        print(\"Inserting GND entities...\")\n",
    "        # Batch insert GND entities in chunks\n",
    "        chunk_size = 1000\n",
    "        for i in range(0, len(gnd_entities), chunk_size):\n",
    "            chunk = gnd_entities[i:i + chunk_size]\n",
    "            cur.executemany(\"\"\"\n",
    "                INSERT INTO dnb_gnd_entities (gnd_idn, gnd_label) \n",
    "                VALUES (%s, %s) \n",
    "                ON CONFLICT (gnd_idn) DO NOTHING\n",
    "            \"\"\", chunk)\n",
    "            print(f\"Processed {min(i + chunk_size, len(gnd_entities))}/{len(gnd_entities)} entities\")\n",
    "            # Commit after each chunk to save progress\n",
    "            cur.connection.commit()\n",
    "    else:\n",
    "        print(\"GND entities already populated, skipping...\")\n",
    "\n",
    "    print(\"Copying records...\")\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO dnb_records_gnd \n",
    "        SELECT * FROM dnb_records \n",
    "        WHERE idn = ANY(%s)\n",
    "        ON CONFLICT DO NOTHING\n",
    "    \"\"\", (doc_idns,))\n",
    "    \n",
    "    print(\"Creating mappings...\")\n",
    "    # First create a temporary table for the mappings\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TEMP TABLE temp_mappings (\n",
    "            doc_idn VARCHAR,\n",
    "            gnd_idn VARCHAR\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert into temp table\n",
    "    cur.executemany(\"\"\"\n",
    "        INSERT INTO temp_mappings (doc_idn, gnd_idn)\n",
    "        VALUES (%s, %s)\n",
    "    \"\"\", mappings)\n",
    "    \n",
    "    # Insert from temp table, eliminating duplicates\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO dnb_gnd_mappings (record_id, gnd_id)\n",
    "        SELECT DISTINCT r.id, e.id\n",
    "        FROM temp_mappings m\n",
    "        JOIN dnb_records_gnd r ON r.idn = m.doc_idn::varchar\n",
    "        JOIN dnb_gnd_entities e ON e.gnd_idn = m.gnd_idn::varchar\n",
    "        ON CONFLICT DO NOTHING\n",
    "    \"\"\")\n",
    "    \n",
    "    # Clean up\n",
    "    cur.execute(\"DROP TABLE temp_mappings\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting process...\")\n",
    "    # Read GND mappings\n",
    "    doc_idns, gnd_entities, mappings = read_gnd_mappings(\"../data/ger_open_access_with_gnd.csv\")\n",
    "    \n",
    "    # Connect to database\n",
    "    print(\"Connecting to database...\")\n",
    "    conn = connect_to_db()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        create_gnd_tables(cur)\n",
    "        populate_tables(cur, doc_idns, gnd_entities, mappings)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nFinal Statistics:\")\n",
    "        cur.execute(\"SELECT COUNT(*) FROM dnb_records_gnd\")\n",
    "        print(f\"Records in subset: {cur.fetchone()[0]}\")\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM dnb_gnd_entities\")\n",
    "        print(f\"GND entities: {cur.fetchone()[0]}\")\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM dnb_gnd_mappings\")\n",
    "        print(f\"Total mappings: {cur.fetchone()[0]}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"\\nDone!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise e\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
